{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4140b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_actions = 18\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7669d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space\n",
    "#we may not need the color? -> turn to grey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_actions = 18\n",
    "q_net = tf.keras.Sequential()\n",
    "q_net.add(tf.keras.layers.Flatten())\n",
    "q_net.add(tf.keras.layers.Dense(hidden_size, activation='relu'))\n",
    "q_net.add(tf.keras.layers.Dense(num_actions, activation='softmax'))\n",
    "#we can try to add convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "e = 0.2\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute the Q values and best action for the current state\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "        \n",
    "        # Epsilon adaptive\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        nst_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        q_values_nst = q_net(np.array([nst_obs]))\n",
    "        action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "        \n",
    "        td_error = q_values.numpy()\n",
    "        td_error[0][action] = reward + gamma*action_nst\n",
    "        loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "        \n",
    "    grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "    trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "    rTot += reward\n",
    "    \n",
    "    obs = nst_obs\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f7444-6515-4c4e-81f2-aaca576c0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rTot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf4649",
   "metadata": {},
   "source": [
    "**Episodes with n_games**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4592930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_steps = 500\n",
    "n_games = 15000\n",
    "E = 1000\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "n_games = 15000\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "for i in range(0, n_games):\n",
    "    print(f'Running game {i+1}/{n_games}...')\n",
    "    e = E / (i + E)\n",
    "    st = game.reset()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "            action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "\n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "\n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*action_nst\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "\n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "# Close the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
