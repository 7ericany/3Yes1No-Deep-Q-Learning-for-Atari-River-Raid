{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory -> D:\\home\\yash\n",
      "$CONDA_PREFIX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# PLEASE SET YOUR OWN WORKING_DIRECTORY WHEN RUNNING LOCALLY\n",
    "WORKING_DIRECTORY = \"/home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/\"\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    WORKING_DIRECTORY = \"/home/yash/working_dir/\"\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX\n",
    "\n",
    "LOG_FILE = WORKING_DIRECTORY + \"log_file.txt\"\n",
    "\n",
    "def write_to_log(statement, include_blank_line=False):\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\") as myfile:\n",
    "            if include_blank_line:\n",
    "                myfile.write(\"\\n\\n\" + statement)\n",
    "            else:\n",
    "                myfile.write(\"\\n\" + statement)\n",
    "    except:\n",
    "        # Running this locally may cause errors, and isn't required\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "D:\\program\\Anaconda\\envs\\atari\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of wrappers to get us started, please use these\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n",
    "        self.GRAYSCALE = GRAYSCALE\n",
    "        self.NORMALIZE = NORMALIZE\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "\n",
    "        \n",
    "        if self.NORMALIZE:\n",
    "            obs = obs / 255.0\n",
    "        # Convert to grayscale -> This isn't quite working right now, but we can update the function quite easily later\n",
    "        if self.GRAYSCALE:\n",
    "#             obs = obs\n",
    "            obs = tf.image.rgb_to_grayscale(obs)\n",
    "                    \n",
    "        image = obs[:,2:-9,8:,:]\n",
    "        image = tf.image.resize(image,[84,84])\n",
    "        image = tf.transpose(tf.reshape(image, image.shape[:-1]),perm = [1,2,0])\n",
    "        return image\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929c1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(env, 4))), GRAYSCALE=True, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c83ee636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "#image = tf.keras.preprocessing.image.array_to_img(tf.reshape(image[:,:,3],[84,84,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image1 = tf.image.resize(obs_,[84,84])\n",
    "# tf.keras.preprocessing.image.array_to_img(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_actions = 18\n",
    "# q_net = tf.keras.Sequential()\n",
    "# q_net.add(tf.keras.layers.Flatten())\n",
    "# q_net.add(tf.keras.layers.Dense(hidden_size, activation='relu'))\n",
    "# q_net.add(tf.keras.layers.Dense(num_actions, activation='softmax'))\n",
    "#we can try to add convolutional layers\n",
    "\n",
    "def create_q_model(input_shape, hidden_size, num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Convolutions on the frames on the screen\n",
    "    \n",
    "    layer1 = layers.Conv2D(16, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(32, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Flatten()(layer2)\n",
    "    layer4 = tf.keras.layers.Dense(hidden_size, activation='relu')(layer3) \n",
    "    action = tf.keras.layers.Dense(num_actions, activation='softmax')(layer4)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "q_net = create_q_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\n",
    "    #hyper parameters\n",
    "    num_steps = 500\n",
    "    obs = env.reset()\n",
    "    rTot = 0\n",
    "    gamma = 0.99\n",
    "    e = 0.2\n",
    "\n",
    "    start_time = time.time()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "            action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "            \n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #SARSA \n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "            if np.random.rand(1) < e:\n",
    "                action_nst = env.action_space.sample()\n",
    "            \n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "            \n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "        \n",
    "        obs = nst_obs\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    end_time = time.time()\n",
    "    inter = end_time-start_time\n",
    "\n",
    "    # Close the env\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07024d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 20.486475944519043\n"
     ]
    }
   ],
   "source": [
    "print(rTot,inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab4b1c",
   "metadata": {},
   "source": [
    "**Episodes with n_game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1053a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "n_games = 1000 #when running in the GCP, we can set a much higher value\n",
    "E = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to log\n",
    "model_name = \"DQN_\" + str(n_games)\n",
    "write_to_log(\"Starting \" + model_name, include_blank_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_list = []\n",
    "time_list = []\n",
    "for i in range(0, n_games):\n",
    "    if (i+1)%200 == 0:\n",
    "        print(f'Running game {i+1}/{n_games}...')\n",
    "        write_to_log(f'Running game {i+1}/{n_games}...', include_blank_line=False)\n",
    "    \n",
    "    e = E / (i + E)\n",
    "    if e < 0.1:\n",
    "        e = 0.1\n",
    "        \n",
    "    obs = env.reset()\n",
    "    start_time = time.time()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "\n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Added an else statement so the command only runs if necessary\n",
    "                action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            #SARSA: policy same as the prev step\n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "            if np.random.rand(1) < e:\n",
    "                action_nst = env.action_space.sample()\n",
    "\n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "\n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    inter = end_time-start_time\n",
    "    rot_list.append(rTot)\n",
    "    time_list.append(inter)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_log(\"Completed \" + model_name, include_blank_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average_reward:{}\".format(sum(rot_list)/n_games))\n",
    "print(\"average_time:{}\".format(sum(time_list)/n_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ed0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_log(\"average_reward:{}\".format(sum(rot_list)/n_games), include_blank_line=False)\n",
    "write_to_log(\"average_time:{}\".format(sum(time_list)/n_games), include_blank_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e807c",
   "metadata": {},
   "source": [
    "1. After 100 games:<br>\n",
    "   average_reward:773.95<br>\n",
    "   average_time:21.668797335624696<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5c291e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534.6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rot_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf0f1afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.960728788375855"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "591376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: model/cDQN_100\\assets\n"
     ]
    }
   ],
   "source": [
    "model_path = WORKING_DIRECTORY + \"model/cDQN_\" + str(n_games)\n",
    "q_net.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff60ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "q_net_copy = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32d1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_net_copy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b200e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"
  },
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
