{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PLEASE SET YOUR OWN WORKING_DIRECTORY WHEN RUNNING LOCALLY\n",
    "WORKING_DIRECTORY = \"/home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/\"\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    WORKING_DIRECTORY = os.getcwd()\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a7cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of wrappers to get us started, please use these\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n",
    "        self.GRAYSCALE = GRAYSCALE\n",
    "        self.NORMALIZE = NORMALIZE\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "\n",
    "        \n",
    "        if self.NORMALIZE:\n",
    "            obs = obs / 255.0\n",
    "        # Convert to grayscale -> This isn't quite working right now, but we can update the function quite easily later\n",
    "        if self.GRAYSCALE:\n",
    "#             obs = obs\n",
    "            obs = tf.image.rgb_to_grayscale(obs)\n",
    "                    \n",
    "        image = obs[:,2:-9,8:,:]\n",
    "        image = tf.image.resize(image,[84,84])\n",
    "        image = tf.transpose(tf.reshape(image, image.shape[:-1]),perm = [1,2,0])\n",
    "        return image\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "929c1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(env, 4))), GRAYSCALE=True, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4b71bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "#image = tf.keras.preprocessing.image.array_to_img(tf.reshape(image[:,:,3],[84,84,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f8f47b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAIAAACTCYeWAAACpElEQVR4nO3VvWsacRjA8ccQe018CemdRNSEmBdCaDuVu9O6SJAMDmbIkElwFLJahwz5A7I0QyC7gS7GKHJQaFcJZmkklWYIQWqa2KYieF40kIAdAlbiVNrfXeF5PpM+eN7z5bzT9ObdMjDz8e3mnx7ybKryanWPxTKDhvQ5zf+J4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxonisKB4riseK4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxonisKB4riseK4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxMiWTSaN3MMyw1+s1eofHYrH3+/tBSTptNOyl0hy7E6H+2aOON+3u7hq9g2GGmX67oig8z6dSqbW1NY7jBj9QLBZ9Pt/gPJPJSJLUaDTi8Ti79djGA4Df70+lUqIoWiyW/vnW1lYymaxUKn6/f/AoRVEWFhYuLi6Y7qbHPR+Lxcrlsqqq/cNutwsAoVAokUj0z1VVPTo62t5+vrrqYb2YHvH5fF6WZbvd3j+MRCIHBwfpdHpycnJnZ6c3v7q6kmU5GmX4D9dj2NM+m8263e719fWzs7NarWbIDszveQAQRTGXywUCAYfD0RtubGw8vAiFQkNDv68Bz/M6rPSA7V+dpmmapv3NNzidzn+1zCC2V95qtVqt1v4Jx93ZbO3e23p9DAAEofloog/97vnFxa/hcNFma7tc9XC46HQ25ue/AUAwWBKEpstVn5u7NJvvx8ZufL4vHHc3Pt7yeH4yXUm/+KmpH4LQtNtvTk5mAWB6+jsARKMfCoWXknQ6M1Obnb3keXV09Pb4eN5svl9ZKSwtfWK6kh4PvAfX1+Pn5+52+6ksn1arE5o20mqN7O0tA0C1OtHpcBZLR1Utt7dPAoHPh4cvFOV1/w3Cwi+lurN9DSnl/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x84>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = tf.image.resize(obs_,[84,84])\n",
    "tf.keras.preprocessing.image.array_to_img(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_actions = 18\n",
    "# q_net = tf.keras.Sequential()\n",
    "# q_net.add(tf.keras.layers.Flatten())\n",
    "# q_net.add(tf.keras.layers.Dense(hidden_size, activation='relu'))\n",
    "# q_net.add(tf.keras.layers.Dense(num_actions, activation='softmax'))\n",
    "#we can try to add convolutional layers\n",
    "\n",
    "def create_q_model(input_shape, hidden_size, num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Convolutions on the frames on the screen\n",
    "    \n",
    "    layer1 = layers.Conv2D(16, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(32, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Flatten()(layer2)\n",
    "    layer4 = tf.keras.layers.Dense(hidden_size, activation='relu')(layer3) \n",
    "    action = tf.keras.layers.Dense(num_actions, activation='softmax')(layer4)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "q_net = create_q_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "e = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(num_steps):\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute the Q values and best action for the current state\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "        \n",
    "        # Epsilon adaptive\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        nst_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        q_values_nst = q_net(np.array([nst_obs]))\n",
    "        action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "        \n",
    "        td_error = q_values.numpy()\n",
    "        td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "        loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "        \n",
    "    grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "    trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "    rTot += reward\n",
    "    \n",
    "    obs = nst_obs\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "end_time = time.time()\n",
    "inter = end_time-start_time\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f29b8",
   "metadata": {},
   "source": [
    "**Episodes with n_game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c61438eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running game 1/10...\n",
      "Running game 2/10...\n",
      "Running game 3/10...\n",
      "Running game 4/10...\n",
      "Running game 5/10...\n",
      "Running game 6/10...\n",
      "Running game 7/10...\n",
      "Running game 8/10...\n",
      "Running game 9/10...\n",
      "Running game 10/10...\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "n_games = 10\n",
    "E = 1000\n",
    "\n",
    "rot_list = []\n",
    "time_list = []\n",
    "for i in range(0, n_games):\n",
    "    print(f'Running game {i+1}/{n_games}...')\n",
    "    \n",
    "    e = E / (i + E)\n",
    "    if e < 0.1:\n",
    "        e = 0.1\n",
    "        \n",
    "    obs = env.reset()\n",
    "    start_time = time.time()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "            action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "\n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "\n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "\n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    inter = end_time-start_time\n",
    "    rot_list.append(rTot)\n",
    "    time_list.append(inter)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "babba91e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (665304405.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Adia Hong\\AppData\\Local\\Temp\\ipykernel_15648\\665304405.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(\"average_time:{}\".format(sum(time_list)/n_games)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"average_reward:{}\".format(sum(rot_list)/n_games)\n",
    "print(\"average_time:{}\".format(sum(time_list)/n_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "591376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/model_1_trial/assets\n"
     ]
    }
   ],
   "source": [
    "q_net.save(WORKING_DIRECTORY + \"model_1_trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff60ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "q_net_copy = keras.models.load_model(WORKING_DIRECTORY + \"model_1_trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f32d1f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.0506279e-03,  2.3967847e-03, -2.2423535e-03, ...,\n",
       "         -1.1952850e-04, -1.7089252e-03, -3.0145252e-03],\n",
       "        [ 3.2449388e-03,  3.4740660e-04,  1.0411497e-03, ...,\n",
       "         -9.4713503e-04,  2.6586130e-03,  2.3412569e-03],\n",
       "        [ 1.9532652e-03,  2.8169909e-03,  1.6335221e-03, ...,\n",
       "         -3.5981813e-03,  7.2118593e-04, -3.0723370e-03],\n",
       "        ...,\n",
       "        [-1.0799554e-03,  3.8140146e-03, -1.9022273e-03, ...,\n",
       "         -4.8977369e-04, -2.5845249e-03,  1.5689335e-03],\n",
       "        [-3.0405628e-03, -3.2310425e-03, -3.0168858e-03, ...,\n",
       "         -2.2355788e-03,  2.0216224e-03, -1.7025322e-03],\n",
       "        [-2.4454824e-03, -1.8618915e-03, -1.6140898e-03, ...,\n",
       "         -5.9294142e-04,  2.6996946e-05,  1.0203468e-03]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[-0.08768681,  0.05589975, -0.05467635, ..., -0.01446234,\n",
       "         -0.06121154,  0.09338515],\n",
       "        [ 0.00138189,  0.00445171, -0.09758045, ..., -0.09112149,\n",
       "         -0.07290541,  0.1981069 ],\n",
       "        [-0.01026739,  0.12270264, -0.04503931, ..., -0.05727364,\n",
       "         -0.1167277 ,  0.00123733],\n",
       "        ...,\n",
       "        [-0.22105888,  0.07109345,  0.22282575, ..., -0.08100915,\n",
       "         -0.01948003,  0.03750946],\n",
       "        [-0.10167394,  0.11500759, -0.12902483, ...,  0.16082896,\n",
       "         -0.14549005,  0.0811583 ],\n",
       "        [-0.049668  , -0.21955656, -0.20658132, ..., -0.10089976,\n",
       "         -0.12028841, -0.05619298]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_net_copy.get_weights()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"
  },
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
