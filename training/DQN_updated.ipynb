{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PLEASE SET YOUR OWN WORKING_DIRECTORY WHEN RUNNING LOCALLY\n",
    "WORKING_DIRECTORY = \"/home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/\"\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    WORKING_DIRECTORY = os.getcwd()\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of wrappers to get us started, please use these\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n",
    "        self.GRAYSCALE = GRAYSCALE\n",
    "        self.NORMALIZE = NORMALIZE\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "        if self.NORMALIZE:\n",
    "            obs = obs / 255.0\n",
    "        # Convert to grayscale -> This isn't quite working right now, but we can update the function quite easily later\n",
    "        if self.GRAYSCALE:\n",
    "            obs = np.dot(obs[...,:3], [0.299, 0.587, 0.114])\n",
    "        return obs\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929c1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(env, 4))), GRAYSCALE=False, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_actions = 18\n",
    "# q_net = tf.keras.Sequential()\n",
    "# q_net.add(tf.keras.layers.Flatten())\n",
    "# q_net.add(tf.keras.layers.Dense(hidden_size, activation='relu'))\n",
    "# q_net.add(tf.keras.layers.Dense(num_actions, activation='softmax'))\n",
    "#we can try to add convolutional layers\n",
    "\n",
    "def create_q_model(input_shape, hidden_size, num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    # layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    # layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    # layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer1 = layers.Flatten()(inputs)\n",
    "    \n",
    "    layer2 = tf.keras.layers.Dense(hidden_size, activation='relu')(layer1) \n",
    "    action = tf.keras.layers.Dense(num_actions, activation='softmax')(layer2)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "q_net = create_q_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0f50642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 9.2445360e-04, -5.0038588e-04,  1.8502246e-03, ...,\n",
       "          3.2054186e-03, -1.0457470e-03,  2.6690653e-03],\n",
       "        [ 4.5484910e-04,  2.6876340e-03,  1.1422448e-03, ...,\n",
       "         -2.7786223e-03,  2.6721102e-03, -1.2223334e-03],\n",
       "        [ 7.2598457e-05,  3.5621896e-03, -2.6043085e-03, ...,\n",
       "          1.9543828e-03, -2.7319037e-03,  3.7012389e-05],\n",
       "        ...,\n",
       "        [-1.6887188e-03, -2.5809696e-03,  1.7139334e-03, ...,\n",
       "          1.0453900e-03, -5.4468424e-04, -3.0088935e-03],\n",
       "        [ 3.1211260e-03, -1.3501954e-03,  1.6895821e-05, ...,\n",
       "          1.5446688e-03, -2.8549910e-03, -3.1436065e-03],\n",
       "        [-3.9374665e-04, -1.6063054e-03,  1.9851960e-03, ...,\n",
       "          2.8538830e-03, -2.8794259e-03, -3.0444178e-03]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[-0.20374604, -0.04329903,  0.18794169, ..., -0.10636999,\n",
       "          0.04963897,  0.02570401],\n",
       "        [ 0.00190172,  0.10242806,  0.10296138, ...,  0.20583339,\n",
       "          0.12973116,  0.00685701],\n",
       "        [ 0.12158315, -0.1717991 , -0.08849593, ...,  0.13527034,\n",
       "          0.12844925,  0.00992671],\n",
       "        ...,\n",
       "        [-0.05558455, -0.02938367, -0.03149539, ..., -0.21260992,\n",
       "         -0.2208755 ,  0.10339637],\n",
       "        [-0.0791292 , -0.13784754,  0.14859335, ...,  0.17996715,\n",
       "         -0.1207506 , -0.15993518],\n",
       "        [ 0.07548963, -0.12712473, -0.1430183 , ..., -0.04838315,\n",
       "          0.00169925,  0.03965069]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_net.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "e = 0.2\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute the Q values and best action for the current state\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "        \n",
    "        # Epsilon adaptive\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        nst_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        q_values_nst = q_net(np.array([nst_obs]))\n",
    "        action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "        \n",
    "        td_error = q_values.numpy()\n",
    "        td_error[0][action] = reward + gamma*action_nst\n",
    "        loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "        \n",
    "    grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "    trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "    rTot += reward\n",
    "    \n",
    "    obs = nst_obs\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9e5e20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.functional.Functional"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(q_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "591376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/model_1_trial/assets\n"
     ]
    }
   ],
   "source": [
    "q_net.save(WORKING_DIRECTORY + \"model_1_trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff60ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "q_net_copy = keras.models.load_model(WORKING_DIRECTORY + \"model_1_trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f32d1f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.0506279e-03,  2.3967847e-03, -2.2423535e-03, ...,\n",
       "         -1.1952850e-04, -1.7089252e-03, -3.0145252e-03],\n",
       "        [ 3.2449388e-03,  3.4740660e-04,  1.0411497e-03, ...,\n",
       "         -9.4713503e-04,  2.6586130e-03,  2.3412569e-03],\n",
       "        [ 1.9532652e-03,  2.8169909e-03,  1.6335221e-03, ...,\n",
       "         -3.5981813e-03,  7.2118593e-04, -3.0723370e-03],\n",
       "        ...,\n",
       "        [-1.0799554e-03,  3.8140146e-03, -1.9022273e-03, ...,\n",
       "         -4.8977369e-04, -2.5845249e-03,  1.5689335e-03],\n",
       "        [-3.0405628e-03, -3.2310425e-03, -3.0168858e-03, ...,\n",
       "         -2.2355788e-03,  2.0216224e-03, -1.7025322e-03],\n",
       "        [-2.4454824e-03, -1.8618915e-03, -1.6140898e-03, ...,\n",
       "         -5.9294142e-04,  2.6996946e-05,  1.0203468e-03]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[-0.08768681,  0.05589975, -0.05467635, ..., -0.01446234,\n",
       "         -0.06121154,  0.09338515],\n",
       "        [ 0.00138189,  0.00445171, -0.09758045, ..., -0.09112149,\n",
       "         -0.07290541,  0.1981069 ],\n",
       "        [-0.01026739,  0.12270264, -0.04503931, ..., -0.05727364,\n",
       "         -0.1167277 ,  0.00123733],\n",
       "        ...,\n",
       "        [-0.22105888,  0.07109345,  0.22282575, ..., -0.08100915,\n",
       "         -0.01948003,  0.03750946],\n",
       "        [-0.10167394,  0.11500759, -0.12902483, ...,  0.16082896,\n",
       "         -0.14549005,  0.0811583 ],\n",
       "        [-0.049668  , -0.21955656, -0.20658132, ..., -0.10089976,\n",
       "         -0.12028841, -0.05619298]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_net_copy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e72f7444-6515-4c4e-81f2-aaca576c0fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rTot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf4649",
   "metadata": {},
   "source": [
    "**Episodes with n_games**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4592930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_steps = 500\n",
    "n_games = 15000\n",
    "E = 1000\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "n_games = 15000\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "for i in range(0, n_games):\n",
    "    print(f'Running game {i+1}/{n_games}...')\n",
    "    e = E / (i + E)\n",
    "    st = game.reset()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "            action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "\n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "\n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*action_nst\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "\n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "# Close the env\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"
  },
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
