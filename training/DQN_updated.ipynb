{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PLEASE SET YOUR OWN WORKING_DIRECTORY WHEN RUNNING LOCALLY\n",
    "WORKING_DIRECTORY = \"/home/yash/Desktop/Courses/CS2470/Final_Project/working_dir/\"\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    WORKING_DIRECTORY = \"/home/yash/working_dir/\"\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a7cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of wrappers to get us started, please use these\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n",
    "        self.GRAYSCALE = GRAYSCALE\n",
    "        self.NORMALIZE = NORMALIZE\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "\n",
    "        \n",
    "        if self.NORMALIZE:\n",
    "            obs = obs / 255.0\n",
    "        # Convert to grayscale -> This isn't quite working right now, but we can update the function quite easily later\n",
    "        if self.GRAYSCALE:\n",
    "#             obs = obs\n",
    "            obs = tf.image.rgb_to_grayscale(obs)\n",
    "                    \n",
    "        image = obs[:,2:-9,8:,:]\n",
    "        image = tf.image.resize(image,[84,84])\n",
    "        image = tf.transpose(tf.reshape(image, image.shape[:-1]),perm = [1,2,0])\n",
    "        return image\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "929c1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(env, 4))), GRAYSCALE=True, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c83ee636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "#image = tf.keras.preprocessing.image.array_to_img(tf.reshape(image[:,:,3],[84,84,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f8f47b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAIAAACTCYeWAAACpElEQVR4nO3VvWsacRjA8ccQe018CemdRNSEmBdCaDuVu9O6SJAMDmbIkElwFLJahwz5A7I0QyC7gS7GKHJQaFcJZmkklWYIQWqa2KYieF40kIAdAlbiVNrfXeF5PpM+eN7z5bzT9ObdMjDz8e3mnx7ybKryanWPxTKDhvQ5zf+J4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxonisKB4riseK4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxonisKB4riseK4rGieKwoHiuKx4risaJ4rCgeK4rHiuKxMiWTSaN3MMyw1+s1eofHYrH3+/tBSTptNOyl0hy7E6H+2aOON+3u7hq9g2GGmX67oig8z6dSqbW1NY7jBj9QLBZ9Pt/gPJPJSJLUaDTi8Ti79djGA4Df70+lUqIoWiyW/vnW1lYymaxUKn6/f/AoRVEWFhYuLi6Y7qbHPR+Lxcrlsqqq/cNutwsAoVAokUj0z1VVPTo62t5+vrrqYb2YHvH5fF6WZbvd3j+MRCIHBwfpdHpycnJnZ6c3v7q6kmU5GmX4D9dj2NM+m8263e719fWzs7NarWbIDszveQAQRTGXywUCAYfD0RtubGw8vAiFQkNDv68Bz/M6rPSA7V+dpmmapv3NNzidzn+1zCC2V95qtVqt1v4Jx93ZbO3e23p9DAAEofloog/97vnFxa/hcNFma7tc9XC46HQ25ue/AUAwWBKEpstVn5u7NJvvx8ZufL4vHHc3Pt7yeH4yXUm/+KmpH4LQtNtvTk5mAWB6+jsARKMfCoWXknQ6M1Obnb3keXV09Pb4eN5svl9ZKSwtfWK6kh4PvAfX1+Pn5+52+6ksn1arE5o20mqN7O0tA0C1OtHpcBZLR1Utt7dPAoHPh4cvFOV1/w3Cwi+lurN9DSnl/wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x84>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = tf.image.resize(obs_,[84,84])\n",
    "tf.keras.preprocessing.image.array_to_img(image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "**DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_actions = 18\n",
    "# q_net = tf.keras.Sequential()\n",
    "# q_net.add(tf.keras.layers.Flatten())\n",
    "# q_net.add(tf.keras.layers.Dense(hidden_size, activation='relu'))\n",
    "# q_net.add(tf.keras.layers.Dense(num_actions, activation='softmax'))\n",
    "#we can try to add convolutional layers\n",
    "\n",
    "def create_q_model(input_shape, hidden_size, num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Convolutions on the frames on the screen\n",
    "    \n",
    "    layer1 = layers.Conv2D(16, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(32, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Flatten()(layer2)\n",
    "    layer4 = tf.keras.layers.Dense(hidden_size, activation='relu')(layer3) \n",
    "    action = tf.keras.layers.Dense(num_actions, activation='softmax')(layer4)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "q_net = create_q_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tf.keras.optimizers.SGD(learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "e = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(num_steps):\n",
    "\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute the Q values and best action for the current state\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "        \n",
    "        # Epsilon adaptive\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        nst_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        q_values_nst = q_net(np.array([nst_obs]))\n",
    "        action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "        \n",
    "        td_error = q_values.numpy()\n",
    "        td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "        loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "        \n",
    "    grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "    trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "    rTot += reward\n",
    "    \n",
    "    obs = nst_obs\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "end_time = time.time()\n",
    "inter = end_time-start_time\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab4b1c",
   "metadata": {},
   "source": [
    "**Episodes with n_game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "745bed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running game 1/100...\n",
      "Running game 2/100...\n",
      "Running game 3/100...\n",
      "Running game 4/100...\n",
      "Running game 5/100...\n",
      "Running game 6/100...\n",
      "Running game 7/100...\n",
      "Running game 8/100...\n",
      "Running game 9/100...\n",
      "Running game 10/100...\n",
      "Running game 11/100...\n",
      "Running game 12/100...\n",
      "Running game 13/100...\n",
      "Running game 14/100...\n",
      "Running game 15/100...\n",
      "Running game 16/100...\n",
      "Running game 17/100...\n",
      "Running game 18/100...\n",
      "Running game 19/100...\n",
      "Running game 20/100...\n",
      "Running game 21/100...\n",
      "Running game 22/100...\n",
      "Running game 23/100...\n",
      "Running game 24/100...\n",
      "Running game 25/100...\n",
      "Running game 26/100...\n",
      "Running game 27/100...\n",
      "Running game 28/100...\n",
      "Running game 29/100...\n",
      "Running game 30/100...\n",
      "Running game 31/100...\n",
      "Running game 32/100...\n",
      "Running game 33/100...\n",
      "Running game 34/100...\n",
      "Running game 35/100...\n",
      "Running game 36/100...\n",
      "Running game 37/100...\n",
      "Running game 38/100...\n",
      "Running game 39/100...\n",
      "Running game 40/100...\n",
      "Running game 41/100...\n",
      "Running game 42/100...\n",
      "Running game 43/100...\n",
      "Running game 44/100...\n",
      "Running game 45/100...\n",
      "Running game 46/100...\n",
      "Running game 47/100...\n",
      "Running game 48/100...\n",
      "Running game 49/100...\n",
      "Running game 50/100...\n",
      "Running game 51/100...\n",
      "Running game 52/100...\n",
      "Running game 53/100...\n",
      "Running game 54/100...\n",
      "Running game 55/100...\n",
      "Running game 56/100...\n",
      "Running game 57/100...\n",
      "Running game 58/100...\n",
      "Running game 59/100...\n",
      "Running game 60/100...\n",
      "Running game 61/100...\n",
      "Running game 62/100...\n",
      "Running game 63/100...\n",
      "Running game 64/100...\n",
      "Running game 65/100...\n",
      "Running game 66/100...\n",
      "Running game 67/100...\n",
      "Running game 68/100...\n",
      "Running game 69/100...\n",
      "Running game 70/100...\n",
      "Running game 71/100...\n",
      "Running game 72/100...\n",
      "Running game 73/100...\n",
      "Running game 74/100...\n",
      "Running game 75/100...\n",
      "Running game 76/100...\n",
      "Running game 77/100...\n",
      "Running game 78/100...\n",
      "Running game 79/100...\n",
      "Running game 80/100...\n",
      "Running game 81/100...\n",
      "Running game 82/100...\n",
      "Running game 83/100...\n",
      "Running game 84/100...\n",
      "Running game 85/100...\n",
      "Running game 86/100...\n",
      "Running game 87/100...\n",
      "Running game 88/100...\n",
      "Running game 89/100...\n",
      "Running game 90/100...\n",
      "Running game 91/100...\n",
      "Running game 92/100...\n",
      "Running game 93/100...\n",
      "Running game 94/100...\n",
      "Running game 95/100...\n",
      "Running game 96/100...\n",
      "Running game 97/100...\n",
      "Running game 98/100...\n",
      "Running game 99/100...\n",
      "Running game 100/100...\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500\n",
    "obs = env.reset()\n",
    "rTot = 0\n",
    "gamma = 0.99\n",
    "n_games = 1000 #when running in the GCP, we can set a much higher value\n",
    "E = 1000\n",
    "\n",
    "rot_list = []\n",
    "time_list = []\n",
    "for i in range(0, n_games):\n",
    "    if (i+1)%25 == 0:\n",
    "        print(f'Running game {i+1}/{n_games}...')\n",
    "    \n",
    "    e = E / (i + E)\n",
    "    if e < 0.1:\n",
    "        e = 0.1\n",
    "        \n",
    "    obs = env.reset()\n",
    "    start_time = time.time()\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute the Q values and best action for the current state\n",
    "            q_values = q_net(np.array([obs]))\n",
    "\n",
    "            # Epsilon adaptive\n",
    "            if np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Added an else statement so the command only runs if necessary\n",
    "                action = tf.math.argmax(q_values,axis=1).numpy()[0]\n",
    "\n",
    "            # apply the action\n",
    "            nst_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "            q_values_nst = q_net(np.array([nst_obs]))\n",
    "            action_nst = tf.math.argmax(q_values_nst,axis=1).numpy()[0]\n",
    "\n",
    "            td_error = q_values.numpy()\n",
    "            td_error[0][action] = reward + gamma*np.max(q_values_nst)\n",
    "            loss = tf.reduce_sum(tf.square(td_error-q_values))\n",
    "\n",
    "        grad = tape.gradient(loss,q_net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grad,q_net.trainable_variables))\n",
    "        rTot += reward\n",
    "\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    inter = end_time-start_time\n",
    "    rot_list.append(rTot)\n",
    "    time_list.append(inter)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average_reward:{}\".format(sum(rot_list)/n_games))\n",
    "print(\"average_time:{}\".format(sum(time_list)/n_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e807c",
   "metadata": {},
   "source": [
    "1. After 100 games:<br>\n",
    "   average_reward:773.95<br>\n",
    "   average_time:21.668797335624696<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5c291e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534.6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rot_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf0f1afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.960728788375855"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "591376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: model/cDQN_100\\assets\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model/cDQN_\" + str(n_games)\n",
    "q_net.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff60ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "q_net_copy = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32d1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_net_copy.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b200e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"
  },
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
