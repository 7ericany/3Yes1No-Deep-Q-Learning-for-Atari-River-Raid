{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c205ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_LOCALLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6642660a-1238-488f-be01-63a2f2e3325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# PLEASE SET YOUR OWN WORKING_DIRECTORY WHEN RUNNING LOCALLY\n",
    "WORKING_DIRECTORY = \"../\"\n",
    "\n",
    "if not RUNNING_LOCALLY:\n",
    "    os.chdir(\"/home/yash/\")\n",
    "    print(\"Current Directory ->\", os.getcwd())\n",
    "\n",
    "    WORKING_DIRECTORY = \"/home/yash/working_dir/\"\n",
    "\n",
    "    # Ensure that you are working in the right environment\n",
    "    !echo $CONDA_PREFIX\n",
    "\n",
    "LOG_FILE = WORKING_DIRECTORY + \"log_file.txt\"\n",
    "\n",
    "def write_to_log(statement, include_blank_line=False):\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\") as myfile:\n",
    "            if include_blank_line:\n",
    "                myfile.write(\"\\n\\n\" + statement)\n",
    "            else:\n",
    "                myfile.write(\"\\n\" + statement)\n",
    "    except:\n",
    "        # Running this locally may cause errors, and isn't required\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a7379d-5c58-4894-8c21-8950478d0958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24a5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatObs(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = \\\n",
    "            spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7cfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of wrappers to get us started, please use these\n",
    "class ObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, GRAYSCALE=False, NORMALIZE=False):\n",
    "        self.GRAYSCALE = GRAYSCALE\n",
    "        self.NORMALIZE = NORMALIZE\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        # Normalise observation by 255\n",
    "\n",
    "        \n",
    "        if self.NORMALIZE:\n",
    "            obs = obs / 255.0\n",
    "        # Convert to grayscale -> This isn't quite working right now, but we can update the function quite easily later\n",
    "        if self.GRAYSCALE:\n",
    "#             obs = obs\n",
    "            obs = tf.image.rgb_to_grayscale(obs)\n",
    "                    \n",
    "        image = obs[:,2:-9,8:,:]\n",
    "        image = tf.image.resize(image,[84,84])\n",
    "        image = tf.transpose(tf.reshape(image, image.shape[:-1]),perm = [1,2,0])\n",
    "        return image\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # Clip reward between 0 to 1\n",
    "        return np.clip(reward, 0, 1)\n",
    "    \n",
    "class ActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, action):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38eb8dfc-80eb-4161-8c6c-c472ebaa3d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Riverraid-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929c1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 23:01:14.568795: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(env, 4))), GRAYSCALE=True, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c83ee636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "#image = tf.keras.preprocessing.image.array_to_img(tf.reshape(image[:,:,3],[84,84,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f8f47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (4, 210, 160, 3), uint8)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535de1",
   "metadata": {},
   "source": [
    "## SARSA($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ed81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_actions = env.action_space.n\n",
    "obs_space_shape = [84, 84, 4]\n",
    "learning_rate = 0.1\n",
    "\n",
    "def create_q_model(input_shape, hidden_size, num_actions):\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Convolutions on the frames on the screen\n",
    "    \n",
    "    layer1 = layers.Conv2D(16, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(32, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Flatten()(layer2)\n",
    "    layer4 = tf.keras.layers.Dense(hidden_size, activation='relu')(layer3) \n",
    "    action = tf.keras.layers.Dense(num_actions, activation='softmax')(layer4)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "q_net = create_q_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d833a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elig_trace = np.zeros((obs.shape, num_actions))   # eligibility trace\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af89cd",
   "metadata": {},
   "source": [
    "**One episode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0dfeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(env, q_net, state, epsilon):\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        action = tf.math.argmax(q_values, axis=1).numpy()[0] # greedy\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_policy(q_net, obs, action, reward, nst_obs, next_action, done):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        q_values = q_net(np.array([obs]))\n",
    "        one_hot_action = tf.one_hot([action], num_actions)\n",
    "        predict = tf.reduce_sum(one_hot_action * q_values, axis=1)\n",
    "\n",
    "        q_values_next = q_net(np.array([nst_obs]))\n",
    "        target = reward + (1 - done) * gamma * q_values_next\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(target - predict))\n",
    "\n",
    "    grad = tape.gradient(loss, q_net.trainable_variables)\n",
    "    trainer.apply_gradients(zip(grad, q_net.trainable_variables))\n",
    "    \n",
    "def train_one_episode(q_net, e, gamma, num_steps):\n",
    "    rTot = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        action = choose_action(env, q_net, obs, e)\n",
    "        nst_obs, reward, done, _ = env.step(action)\n",
    "        delta = 1.0 if done else 0.0\n",
    "\n",
    "        next_action = choose_action(env, q_net, nst_obs, e)\n",
    "        update_policy(q_net, obs, action, reward, nst_obs, next_action, done)\n",
    "\n",
    "        rTot += reward\n",
    "        obs = nst_obs\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return rTot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21ca50e5-2251-41c5-b99e-355688a4cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_steps = 500\n",
    "gamma = 0.99\n",
    "e = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "rTot = train_one_episode(q_net, e, gamma, num_steps)\n",
    "end_time = time.time()\n",
    "\n",
    "inter = end_time - start_time\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07024d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0 13.43814992904663\n"
     ]
    }
   ],
   "source": [
    "print(rTot,inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab4b1c",
   "metadata": {},
   "source": [
    "**Episodes with n_game**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1053a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 10 # when running in the GCP, we can set a much higher value\n",
    "E = 1000      # To adjust the epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0eafb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to log\n",
    "model_name = \"SARSA_\" + str(n_games)\n",
    "write_to_log(\"Starting \" + model_name, include_blank_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "745bed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_list = []\n",
    "time_list = []\n",
    "for i in range(n_games):\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f'Running game {i+1}/{n_games}...')\n",
    "        write_to_log(f'Running game {i+1}/{n_games}...', include_blank_line=False)\n",
    "    \n",
    "    e = E / (i + E)\n",
    "    if e < 0.1:\n",
    "        e = 0.1\n",
    "        \n",
    "    start_time = time.time()\n",
    "    rTot = train_one_episode(q_net, e, gamma, num_steps)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inter = end_time-start_time\n",
    "    rot_list.append(rTot)\n",
    "    time_list.append(inter)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "625e8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_reward:14.5\n",
      "average_time:12.447486996650696\n"
     ]
    }
   ],
   "source": [
    "print(\"average_reward:{}\".format(sum(rot_list)/n_games))\n",
    "print(\"average_time:{}\".format(sum(time_list)/n_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98c6bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.0, 14.0, 16.0, 15.0, 12.0, 14.0, 15.0, 15.0, 13.0, 16.0]\n"
     ]
    }
   ],
   "source": [
    "print(rot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_log(\"Completed \" + model_name, include_blank_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ed0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_log(\"average_reward:{}\".format(sum(rot_list)/n_games), include_blank_line=False)\n",
    "write_to_log(\"average_time:{}\".format(sum(time_list)/n_games), include_blank_line=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e807c",
   "metadata": {},
   "source": [
    "1. After 100 games:<br>\n",
    "   average_reward:773.95<br>\n",
    "   average_time:21.668797335624696<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c5c291e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534.6"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rot_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf0f1afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.960728788375855"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list[-5:])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "591376f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: model/cDQN_100\\assets\n"
     ]
    }
   ],
   "source": [
    "model_path = WORKING_DIRECTORY + \"model/cDQN_\" + str(n_games)\n",
    "q_net.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff60ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "q_net_copy = keras.models.load_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "b8bd69ed4ffee0ae412486d98ceaaadb3fa2922e6e180ca66d279a33125fa193"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
