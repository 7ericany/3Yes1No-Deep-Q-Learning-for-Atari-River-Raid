{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4132f4b9",
   "metadata": {},
   "source": [
    "### Jupyter Notebook, contains same code as `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc27409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 23:00:03.554703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 23:00:03.554724: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/yash/Desktop/Courses/CS2470/Final_Project/repo/proj_env_2/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory -> /home/yash\n",
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 23:00:06.689746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 23:00:06.689771: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 23:00:06.689784: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (yash-ThinkPad-L390): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_wrappers import *\n",
    "from utils import *\n",
    "from build_model import build_dq_model\n",
    "from memory import Memory\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afadbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_net, state, epsilon, num_actions):\n",
    "    if epsilon > np.random.rand(1)[0]:\n",
    "        action = np.random.choice(num_actions)\n",
    "    else:\n",
    "        state_t = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = tf.argmax(q_net(state_t, training=False)[0]).numpy()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "2022-05-11 23:00:09.622973: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Using the wrappers for the environment\n",
    "env = gym.make(\"ALE/Riverraid-v5\")\n",
    "env = ObservationWrapper(RewardWrapper(ActionWrapper(ConcatObs(FireResetEnv(env), k=4, DEATH_REWARD=1000)), CLIP=False, SCALE=True), GRAYSCALE=True, NORMALIZE=True)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "hidden_size = 640\n",
    "num_actions = 18\n",
    "q_net = build_dq_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)\n",
    "q_net_target = build_dq_model(input_shape=obs.shape, hidden_size=hidden_size, num_actions=num_actions)\n",
    "loss_func = keras.losses.Huber()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "experience = Memory(length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "discount_factor = 0.99\n",
    "batch_size = 32\n",
    "n_steps_episode = 10000\n",
    "rolling_reward = 0\n",
    "episode_ix = 0\n",
    "observation_ix = 0\n",
    "random_till = 3000\n",
    "eps_decay = 0.9995\n",
    "epsilon = 1.0\n",
    "eps_threshold = 0.1\n",
    "weight_update_freq = 10\n",
    "target_update_freq = 500\n",
    "n_episodes = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913b35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training\n",
    "write_to_log(\"Starting Training for new implementation\", include_blank_line=True)\n",
    "reward_history = np.zeros(n_episodes)\n",
    "num_steps_history = np.zeros(n_episodes)\n",
    "SWITCHED_TO_GREEDY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode_ix < n_episodes:\n",
    "    state = np.array(env.reset())\n",
    "    ep_total_reward = 0\n",
    "\n",
    "    if episode_ix in [500, 2000, 5000, 10000, 20000, 50000]:\n",
    "        # print(\"Episode: \", episode_ix)\n",
    "        write_to_log(\"Saving model at \" + str(episode_ix))\n",
    "        curr_model_name = \"curr_model_rev_\" + str(episode_ix)\n",
    "        curr_model_name_target = \"curr_model_rev_target_\" + str(episode_ix)\n",
    "        save_models_and_arrays(curr_model_name, curr_model_name_target, \n",
    "                            q_net, q_net_target,\n",
    "                            reward_history[:episode_ix], \n",
    "                            num_steps_history[:episode_ix])\n",
    "\n",
    "    for _ in range(1, n_steps_episode):\n",
    "        observation_ix += 1\n",
    "\n",
    "        if observation_ix < random_till:\n",
    "            # Still exploring, not following policy\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Take action as suggested by policy\n",
    "            if not SWITCHED_TO_GREEDY:\n",
    "                write_to_log(\"Switching to Greedy, at observation \", observation_ix)\n",
    "                SWITCHED_TO_GREEDY = True\n",
    "            action = choose_action(q_net, state, epsilon, num_actions)\n",
    "\n",
    "        # Take action and get new observations\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        ep_total_reward += reward\n",
    "\n",
    "        # Update Experience with experience tuple and then update state\n",
    "        experience.append((state, state_next, action, reward, done))\n",
    "        state = state_next\n",
    "\n",
    "        # Decay epsilon till threshold\n",
    "        epsilon = max(epsilon*eps_decay, eps_threshold)\n",
    "\n",
    "        if len(experience) > batch_size and observation_ix % weight_update_freq == 0:\n",
    "\n",
    "            state_sample, state_next_sample, rewards_sample, action_sample, done_sample = \\\n",
    "                experience.sample_memory(batch_size)\n",
    "\n",
    "            # Calculate Q values as sum of reward and discounted future rewards and handle final state\n",
    "            future_rewards = q_net_target.predict(state_next_sample)\n",
    "            updated_q_vals = rewards_sample + (discount_factor * tf.reduce_max(future_rewards, axis=1))\n",
    "            updated_q_vals = updated_q_vals * (1 - done_sample) - done_sample\n",
    "\n",
    "            masked_action = tf.one_hot(tf.cast(action_sample, dtype=tf.int32), num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_vals = q_net(state_sample)\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_vals, masked_action), axis=1)\n",
    "                loss = loss_func(updated_q_vals, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, q_net.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, q_net.trainable_variables))\n",
    "\n",
    "        if observation_ix % target_update_freq == 0:\n",
    "            # Update Target QNet\n",
    "            q_net_target.set_weights(q_net.get_weights())\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(rolling_reward, episode_ix, observation_ix))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history[episode_ix] = ep_total_reward\n",
    "    rolling_reward = np.mean(reward_history[:episode_ix])\n",
    "\n",
    "    episode_ix += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "curr_model_name = \"final_curr_rev_model_\" + str(episode_ix)\n",
    "curr_model_name_target = \"final_curr_model_rev_target_\" + str(episode_ix)\n",
    "save_models_and_arrays(curr_model_name, curr_model_name_target, \n",
    "                    q_net, q_net_target,\n",
    "                    reward_history, num_steps_history)\n",
    "write_to_log(\"Training for new implementation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d20e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "interpreter": {
   "hash": "dd5906573090c30d3b3a0a7f67b6e07d270f26255c1965b490c2bf60d2c96078"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('proj_env_2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
